-annealing_generator.py

import numpy as np
import math
import random
from verifier import compute_m_height
import argparse
import sys # For exit
import signal # For Ctrl+C handling
from tqdm import tqdm # For progress bar
import multiprocessing
import time # For adding slight delay in worker start
import os # Needed for os.getpid() in worker seeding
import itertools
from multiprocessing import Value # For shared counter

# Global flag for Ctrl+C - Primarily for the main process now
user_interrupted = False

# --- Worker Initializer for Shared Counter ---
# Global variable placeholder in the worker process
worker_cost_counter = None

def init_worker_counter(counter):
    """Initializer function for worker processes to inherit the counter."""
    global worker_cost_counter
    worker_cost_counter = counter

def signal_handler(sig, frame):
    """Handle Ctrl+C: set flag and print message."""
    global user_interrupted
    if user_interrupted: # Second Ctrl+C
        print('\nCtrl+C detected again. Exiting forcefully.')
        sys.exit(1)
    print('\nCtrl+C detected. Asking workers to finish current step and stopping...')
    user_interrupted = True
    # Signal handling within workers is more complex; often rely on pool termination

def calculate_cost(G, m):
    """Calculates the m-height, handling potential errors or infinite results."""
    try:
        height = compute_m_height(G, m)
        # Replace inf with a very large number for comparison purposes,
        # but maybe it's better to handle inf directly in acceptance logic.
        # Using inf directly is generally safer.
        return height
    except ValueError as e:
        # This might happen if G becomes unsuitable during mutation,
        # or m is invalid for G. Treat as very high cost.
        # print(f"Warning: compute_m_height failed for G={G} with m={m}. Error: {e}") # Less verbose in parallel
        return float('inf')
    except Exception as e:
        # Catch other potential exceptions from linprog
        # print(f"Unexpected error during cost calculation: {e}") # Less verbose in parallel
        return float('inf')

def get_neighbor(P, element_range):
    """Generates a neighbor P matrix by changing one element randomly,
       ensuring no column becomes all zeros.
    """
    k, p_cols = P.shape
    P_new = P.copy()
    max_retries_neighbor = 10 # Safeguard against unusual situations
    max_retries_value = 20 # Safeguard for finding a non-zero value

    for _ in range(max_retries_neighbor):
        row_idx = random.randrange(k)
        col_idx = random.randrange(p_cols)
        current_val = P_new[row_idx, col_idx]

        possible_values = list(range(element_range[0], element_range[1] + 1))
        if len(possible_values) <= 1:
            # If range is single value, change might be impossible or always make zero col
            continue # Try modifying a different element

        # Try to find a *different* value first
        new_val = current_val
        attempt_val = 0
        while new_val == current_val and attempt_val < max_retries_value:
             new_val = random.choice(possible_values)
             attempt_val += 1

        if new_val == current_val: # Still couldn't find a different value
             # If only one value possible, this is expected. If more, failed to find different.
             continue # Try changing a different element

        # Temporarily make the change
        original_value_in_col = P_new[row_idx, col_idx]
        P_new[row_idx, col_idx] = new_val

        # Check if the column is now all zeros
        if not np.all(P_new[:, col_idx] == 0):
            # found_valid_new_val = True
            return P_new # Found a valid neighbor
        else:
            # Revert the change for this specific value try
            P_new[row_idx, col_idx] = original_value_in_col
            # Continue loop to try a different new_val or (row, col)

        # If no valid new_val found for this (row, col), the outer loop continues
        # to try modifying a different element.

    # If max_retries_neighbor reached without finding a valid move
    # This should be rare with a large element_range
    # print("Warning: get_neighbor failed to find a valid move after multiple retries. Returning original P.") # Less verbose
    return P # Return original if stuck

def simulated_annealing_core(initial_P, k, n, m, T_max, T_min, alpha, iter_per_temp, element_range, worker_id=0, seed=None):
    """Core SA logic for a single run."""
    # Access the counter via the global variable initialized in the worker
    global worker_cost_counter

    if seed is not None:
        random.seed(seed)
        np.random.seed(seed) # Also seed numpy for initial P generation if done here

    if not (1 <= m <= n - 1):
        # This validation should ideally happen before starting workers
        return None, float('inf') # Indicate error

    I_k = np.eye(k, dtype=initial_P.dtype) # Ensure I has same dtype as P

    current_P = initial_P
    current_G = np.hstack((I_k, current_P))
    current_cost = calculate_cost(current_G, m)

    # >>> Increment counter for initial calculation <<<
    if worker_cost_counter is not None:
        with worker_cost_counter.get_lock(): # Use lock for safety
            worker_cost_counter.value += 1

    best_P = current_P
    best_cost = current_cost

    T = T_max

    # print(f"Starting Annealing (Systematic): Initial Cost = {current_cost:.4f}, T_max = {T_max}, T_min = {T_min}, alpha = {alpha}")
    # print("Press Ctrl+C to stop early and get the best result found so far.")

    # Estimate total steps for tqdm progress bar
    # if T_max > T_min and 0 < alpha < 1:
    #     total_steps_estimate = math.ceil(math.log(T_min / T_max) / math.log(alpha))
    # else:
    #     total_steps_estimate = 0

    # Initialize tqdm progress bar
    # pbar = tqdm(total=total_steps_estimate, desc=f"T={T:.2e}, Best={best_cost:.4f}", unit="steps", disable=(total_steps_estimate==0))

    temp_step = 0
    # try: # No longer needed as signal handling is external
    while T > T_min:
        # Check for user interrupt at the beginning of each temperature step
        # Complex to implement reliably within worker, rely on pool termination
        # if user_interrupted:
            # print(f"\nWorker {worker_id} detecting interruption.")
            # break

        for i in range(iter_per_temp):
            neighbor_P = get_neighbor(current_P, element_range)
            neighbor_G = np.hstack((I_k, neighbor_P))
            neighbor_cost = calculate_cost(neighbor_G, m)

            # >>> Increment counter for neighbor calculation <<<
            if worker_cost_counter is not None:
                with worker_cost_counter.get_lock():
                    worker_cost_counter.value += 1

            delta_E = neighbor_cost - current_cost

            # --- Acceptance Logic (same as before, with overflow check) ---
            accept = False
            if neighbor_cost == float('inf') and current_cost == float('inf'):
                accept = False # Cannot improve from inf to inf
            elif neighbor_cost == float('inf'):
                accept = False # Don't accept infinite cost if current is finite
            elif current_cost == float('inf'):
                accept = True # Always accept if moving away from infinite cost
            elif delta_E < 0:
                accept = True
            else:
                 # Avoid math.exp overflow if delta_E / T is huge
                if delta_E == 0: # Avoid 0/T issues if T is very small
                    acceptance_prob = 1.0 # Or 0.0 depending on philosophy, 1.0 allows movement
                elif T <= 0: # Avoid division by zero or negative T
                     acceptance_prob = 0.0
                else:
                    exponent = -delta_E / T
                    if exponent > 700: # math.exp(709) is roughly max float
                        acceptance_prob = 0.0
                    else:
                        acceptance_prob = math.exp(exponent)

                if random.random() < acceptance_prob:
                    accept = True
            # --- End Acceptance Logic ---

            if accept:
                current_P = neighbor_P # Update P
                current_cost = neighbor_cost

                # Update best solution found so far
                if current_cost < best_cost:
                    best_P = current_P # Store the best P
                    best_cost = current_cost
                    # No printing inside core loop for performance
                    # print(f"  Worker {worker_id}: New Best Found: Cost = {best_cost:.4f} at T = {T:.2f}")
                    # Update progress bar description immediately when best changes
                    # if pbar:
                    #     pbar.set_description(f"T={T:.2e}, Best={best_cost:.4f}")

        T *= alpha # Cool down
        temp_step += 1
        # if pbar:
        #     pbar.update(1)
        #     if temp_step % 10 != 0: # Avoid duplicate printing if best changed
        #         pbar.set_description(f"T={T:.2e}, Best={best_cost:.4f}")
        # Print less frequently to console if using progress bar
        # if temp_step % 50 == 0:
        #      print(f"Temp Step {temp_step}: T = {T:.2f}, Current Cost = {current_cost:.4f}, Best Cost = {best_cost:.4f}")

    # finally: # No longer needed
        # Close the progress bar and restore original signal handler
        # if pbar:
            # pbar.close()
        # signal.signal(signal.SIGINT, original_sigint_handler) # Restore handler

    # print(f"\nWorker {worker_id} Annealing Finished. Best Cost Found: {best_cost:.4f}")
    best_G = np.hstack((I_k, best_P)) # Construct final best G
    return best_G, best_cost

def is_valid_P(P):
    """Checks if any column in P is the all-zero vector."""
    k, p_cols = P.shape
    for j in range(p_cols):
        if np.all(P[:, j] == 0):
            return False
    return True

def run_single_annealing_instance(args_tuple):
    """
    Worker function to run one independent simulated annealing process.
    Takes all arguments packed in a tuple. Handles initialization and seeding.
    """
    # Unpack arguments
    # args_tuple contains (worker_id, k, n, m, element_range, T_max, T_min, alpha, iter_per_temp, seed)
    worker_id, k, n, m, element_range, T_max, T_min, alpha, iter_per_temp, seed = args_tuple
    print(f"[Worker {worker_id}] Starting.")

    # Ensure each worker has a different random seed
    # Use a more robust seeding mechanism if strict reproducibility across runs is needed
    if seed is None:
        # Generate a seed if none provided (e.g., based on time, pid, and worker_id)
        base_seed = int(time.time() * 1000) + random.randint(0, 10000) + worker_id + os.getpid()
        seed = base_seed % (2**32) # Ensure seed is within the valid range [0, 2**32 - 1]
    random.seed(seed)
    np.random.seed(seed)
    # print(f"Worker {worker_id} (PID {os.getpid()}) initialized with seed {seed}") # Debug seeding
    print(f"[Worker {worker_id}] Seeded.")

    # Generate initial P matrix *within* the worker to ensure independence
    p_cols = n - k
    initial_P = None
    max_init_retries = 100
    element_min, element_max = element_range
    print(f"[Worker {worker_id}] Generating initial P...")
    for attempt in range(max_init_retries):
        # Ensure element_max+1 is correct for randint upper bound (exclusive)
        P_candidate = np.random.randint(element_min, element_max + 1, size=(k, p_cols))
        if is_valid_P(P_candidate):
            initial_P = P_candidate
            break

    if initial_P is None:
        print(f"[Worker {worker_id}] Error: Failed to generate a valid initial P matrix after {max_init_retries} attempts.")
        # Return failure indicator: None for G, inf cost, and worker_id
        print(f"[Worker {worker_id}] Returning failure.")
        return None, float('inf'), worker_id

    print(f"[Worker {worker_id}] Initial P generated.")

    # Run the core annealing logic
    print(f"[Worker {worker_id}] Starting annealing core...")
    best_generator, min_height = simulated_annealing_core(
        initial_P=initial_P,
        k=k,
        n=n,
        m=m,
        T_max=T_max,
        T_min=T_min,
        alpha=alpha,
        iter_per_temp=iter_per_temp,
        element_range=element_range,
        worker_id=worker_id,
        seed=seed
    )
    print(f"[Worker {worker_id}] Annealing core finished. Cost: {min_height:.4f}")

    # Return result along with worker_id for tracking if needed
    print(f"[Worker {worker_id}] Returning success.")
    return best_generator, min_height, worker_id

if __name__ == "__main__":
    # Required for multiprocessing spawn method on some OS (like Windows)
    # Needs to be at the top level of the `if __name__ == "__main__":` block
    multiprocessing.freeze_support() # No-op on Linux/macOS, necessary for Windows executable

    parser = argparse.ArgumentParser(description="Find a systematic generator matrix G=[I|P] minimizing m-height using Parallel Simulated Annealing.")

    # --- Arguments (mostly unchanged) ---
    parser.add_argument('-k', type=int, default=3, help='Number of message bits (rows). Default: 3')
    parser.add_argument('-n', type=int, default=6, help='Number of codeword bits (columns). Default: 6')
    parser.add_argument('-m', type=int, default=2, help='m-height parameter (must be 1 <= m <= n-1). Default: 2')

    # Updated defaults and help text for P matrix element range
    parser.add_argument('--element-min', type=int, default=-100, help='Minimum value for elements in P matrix. Default: -100')
    parser.add_argument('--element-max', type=int, default=100, help='Maximum value for elements in P matrix. Default: 100')

    parser.add_argument('--t-max', type=float, default=100.0, help='Initial annealing temperature. Default: 100.0')
    parser.add_argument('--t-min', type=float, default=65.0, help='Final annealing temperature. Default: 65.0')
    parser.add_argument('--alpha', type=float, default=0.95, help='Cooling rate (multiplier). Default: 0.95')
    parser.add_argument('--iter-per-temp', type=int, default=1, help='Iterations per temperature level. Default: 1')

    # --- New Argument for Parallelism ---
    try:
        default_workers = multiprocessing.cpu_count()
    except NotImplementedError:
        default_workers = 1 # Fallback if cpu_count fails
        print("Warning: Could not determine CPU count, defaulting to 1 worker.")
    parser.add_argument('--workers', type=int, default=default_workers,
                        help=f'Number of parallel annealing runs. Default: {default_workers} (logical CPU cores)')

    args = parser.parse_args()

    # --- Use Parsed Arguments ---
    K = args.k
    N = args.n
    M = args.m
    ELEMENT_MIN = args.element_min
    ELEMENT_MAX = args.element_max
    ELEMENT_RANGE = (ELEMENT_MIN, ELEMENT_MAX)
    T_MAX = args.t_max
    T_MIN = args.t_min
    ALPHA = args.alpha
    ITER_PER_TEMP = args.iter_per_temp
    NUM_WORKERS = args.workers

    # --- Calculate total cost calls for progress bar ---
    total_cost_calls = 0
    if T_MAX > T_MIN and 0 < ALPHA < 1:
        try:
            num_steps = math.ceil(math.log(T_MIN / T_MAX) / math.log(ALPHA))
            total_cost_calls = NUM_WORKERS * num_steps * ITER_PER_TEMP
            print(f"Expecting approx. {num_steps} temp steps per worker.")
            print(f"Total expected cost evaluations across {NUM_WORKERS} workers: {total_cost_calls}")
        except ValueError: # Catch potential math domain errors (e.g., log(negative))
            print("Warning: Could not calculate total steps, progress bar may be inaccurate.")
            total_cost_calls = 1 # Avoid division by zero if tqdm total is 0
    else:
        print("Warning: T_max <= T_min or alpha >= 1 or alpha <= 0. Progress bar total may be inaccurate.")
        total_cost_calls = 1 # Avoid division by zero if tqdm total is 0

    # --- Create Shared Counter ---
    # Using Value from multiprocessing for process-safe shared integer
    cost_call_counter = Value('i', 0)

    # --- Validate Parameters ---
    if K < 1:
        parser.error(f"k ({K}) must be at least 1")
    if N <= K:
        parser.error(f"n ({N}) must be greater than k ({K}) for a systematic code with P")
    p_cols = N - K # Number of columns in P
    if p_cols < 1:
         parser.error(f"n-k ({p_cols}) must be at least 1, check n and k values")

    if not (1 <= M <= N - 1):
        parser.error(f"m ({M}) must be between 1 and n-1 ({N-1})")
    if ELEMENT_MIN > ELEMENT_MAX:
        parser.error(f"element-min ({ELEMENT_MIN}) cannot be greater than element-max ({ELEMENT_MAX})")
    # Check for invalid range [0, 0] specifically
    if ELEMENT_MIN == 0 and ELEMENT_MAX == 0:
         parser.error(f"Element range is [0, 0]. Cannot satisfy no-all-zero column constraint.")
    # Warnings for potentially problematic ranges (less critical now with is_valid_P)
    # if ELEMENT_MAX < 0 and ELEMENT_MIN < 0:
    #      print("Warning: Element range is entirely negative.")
    # if ELEMENT_MIN > 0 and ELEMENT_MAX > 0:
    #      print("Warning: Element range is entirely positive.")
    if NUM_WORKERS < 1:
        parser.error(f"Number of workers ({NUM_WORKERS}) must be at least 1")

    # --- Initialization (moved inside worker) ---
    # print(f"Generating initial random P matrix ({K}x{p_cols})...\") # Done by workers

    # I_k = np.eye(K, dtype=initial_P.dtype) # Dtype determined by worker's P
    # initial_generator = np.hstack((I_k, initial_P)) # Example G no longer relevant here

    # print("Initial Random Systematic Generator Matrix G = [I|P]:") # Not applicable here
    # print(initial_generator) # Not applicable here
    print(f"Starting Parallel Simulated Annealing with {NUM_WORKERS} workers.")
    print(f"Targeting m={M}-height minimization for a systematic ({K}x{N}) matrix.")
    print(f"Element range for P: [{ELEMENT_RANGE[0]}, {ELEMENT_RANGE[1]}]")
    print(f"Params: T_max={T_MAX}, T_min={T_MIN}, alpha={ALPHA}, iter_per_temp={ITER_PER_TEMP}")
    print("Press Ctrl+C to stop early (may take a moment to terminate workers).")

    # Prepare arguments for each worker
    # Each worker gets a unique ID (0 to NUM_WORKERS-1) and None seed (will generate its own)
    # Counter is no longer included here
    worker_args = [
        (i, K, N, M, ELEMENT_RANGE, T_MAX, T_MIN, ALPHA, ITER_PER_TEMP, None)
        for i in range(NUM_WORKERS)
    ]

    # --- Setup Signal Handler for Main Process ---
    original_sigint_handler = signal.getsignal(signal.SIGINT)
    # Set the global flag handler
    signal.signal(signal.SIGINT, signal_handler)

    pool = None # Define pool outside try block for finally clause
    results = []
    try:
        # Initialize the pool with the worker initializer function and the counter
        with multiprocessing.Pool(processes=NUM_WORKERS,
                                  initializer=init_worker_counter,
                                  initargs=(cost_call_counter,)) as pool:
            # Use imap_unordered: processes results as they complete, potentially better memory usage
            # than map, order doesn't matter since we find the min cost later.
            # No wrapper needed now, just pass the worker function and args (without counter)
            result_iterator = pool.imap_unordered(run_single_annealing_instance, worker_args)

            # Process results as they come in using tqdm for overall progress
            print(f"Launching {NUM_WORKERS} annealing runs...")
            # Use total_cost_calls for the progress bar total
            pbar = tqdm(total=total_cost_calls, desc="Cost Evals Progress", unit="evals", smoothing=0.1)
            processed_count = 0

            while processed_count < NUM_WORKERS:
                # Check for interrupt FIRST, before waiting for a result
                if user_interrupted:
                    print(f"\nInterrupt detected by main process. Breaking result loop...")
                    break # Exit the main 'while processed_count < NUM_WORKERS' loop

                try:
                    # Wait for the next result. Use a timeout to update progress bar.
                    result = result_iterator.next(timeout=0.5)

                    # Process result ONLY if not interrupted
                    best_G, cost, worker_id = result
                    if best_G is not None:
                        results.append((best_G, cost))
                    else:
                        print(f"Worker {worker_id} failed to produce a valid result.")
                    processed_count += 1
                    # Update progress bar display without incrementing count (n is updated below)
                    pbar.update(0)
                except StopIteration:
                    print("\nResult iterator exhausted.") # Debug print
                    # This happens when the iterator is exhausted normally
                    break # Exit loop if all results are processed
                except KeyboardInterrupt: # Should be caught by the outer try/except
                    print("\nKeyboardInterrupt caught directly in loop.")
                    user_interrupted = True # Ensure flag is set
                    break
                except multiprocessing.TimeoutError:
                    # Timeout occurred, means no worker finished in the last 0.5s
                    # Update progress bar with the current counter value
                    pbar.n = cost_call_counter.value
                    pbar.refresh()
                    continue # Continue loop to check interrupt/wait again
                except Exception as worker_exc: # Catch potential errors from workers
                    print(f"\nError processing result from a worker: {worker_exc}")
                    # Decide how to handle worker errors, e.g., log and continue or stop
                    processed_count += 1 # Count it as processed (failed)
                    pbar.update(1) # Update progress bar even on error

                # Update progress bar after processing a result or catching timeout
                # Ensure the bar reflects the latest count
                pbar.n = cost_call_counter.value
                pbar.refresh()

            pbar.close() # Close progress bar

            # --- Pool Shutdown Logic ---
            if user_interrupted:
                print("\nInterrupt confirmed post-loop. Terminating running worker processes...")
                # Forcefully terminate workers instead of waiting with join()
                pool.terminate()
                # We still need to join to wait for the termination to complete
                pool.join()
                print("Worker pool terminated.")
            else:
                # This path is taken if the loop finished because processed_count == NUM_WORKERS
                print("\nAll scheduled workers completed or failed.")
                # Pool closes and joins automatically via context manager exit
                # if no interrupt occurred.

    except KeyboardInterrupt:
        # This catches Ctrl+C if it happens outside the pool context or during setup/cleanup
        print("\nKeyboardInterrupt caught in main process.")
        # Pool termination is handled by the context manager ('with Pool(...)') exiting
        # or explicitly if needed before exiting the program
        sys.exit(1) # Exit after interrupt
    except Exception as e:
        print(f"\nAn unexpected error occurred in the main process: {e}")
        import traceback
        traceback.print_exc()
        # Pool termination handled by context manager
        sys.exit(1) # Exit on error
    finally:
        # Restore original signal handler *after* pool is guaranteed to be closed
        signal.signal(signal.SIGINT, original_sigint_handler)
        # Ensure pool is terminated if loop exited prematurely without context manager finishing
        if pool is not None and not user_interrupted: # Avoid terminate if already handled by interrupt logic
             # This check might be redundant with the context manager, but safer
             # pool.terminate()
             # pool.join()
             pass

    # --- Aggregate Results ---
    if not results:
        print("\nNo valid results were obtained from any worker.")
        if user_interrupted:
             print("Annealing process was interrupted.")
        sys.exit(1)

    # Find the best result among all workers
    results.sort(key=lambda x: x[1]) # Sort by cost (min_height)
    overall_best_G, overall_best_cost = results[0]

    print("\n--- Overall Best Result ---")
    print(f"Obtained from {len(results)} successful worker run(s).")
    print("Best Systematic Generator Matrix Found G = [I|P]:")
    # Limit printing precision for readability
    with np.printoptions(precision=3, suppress=True):
         print(overall_best_G)
    # Use .g format for cost to handle potentially very small or large numbers well
    print(f"Minimal m-height (m={M}): {overall_best_cost:.6g}")

    # Optional: Verify the final cost again
    final_cost_check = calculate_cost(overall_best_G, M)
    print(f"Verification of final cost: {final_cost_check:.6g}")

    if final_cost_check != overall_best_cost:
         # Added check for floating point comparison issues or potential bugs
         print(f"Warning: Final cost verification ({final_cost_check:.6g}) differs slightly from recorded best cost ({overall_best_cost:.6g}).")

    if user_interrupted:
        print("\nProcess was interrupted. The reported best result is based on completed runs before interruption.")

-brute_force_generator.py

import numpy as np
import math
import itertools
import argparse
import sys
import threading
import time
import os
try:
    # Assuming compute_m_height is in verifier.py at the same level
    from verifier import compute_m_height
except ImportError:
    print("Error: Could not import 'compute_m_height' from 'verifier.py'.")
    print("Ensure 'verifier.py' exists and is in the same directory or Python path.")
    sys.exit(1)

# Global variables for best result and lock
best_P_global = None
min_cost_global = float('inf')
lock = threading.Lock()
cost_counter = 0 # Simple counter (approximate in threaded env)
cost_counter_lock = threading.Lock() # Lock for accurate counting if needed, but adds overhead

def calculate_cost(G, m):
    """Calculates the m-height, incrementing counter, handling errors."""
    global cost_counter, cost_counter_lock
    # Increment counter (using lock for accuracy, remove lock for less overhead)
    with cost_counter_lock:
        cost_counter += 1
    try:
        # Assuming compute_m_height handles potential LinAlgError etc.
        height = compute_m_height(G, m)
        # Ensure infinity is returned consistently if calculation fails
        return height if np.isfinite(height) else float('inf')
    except Exception as e:
        # print(f"Warning: compute_m_height failed. Error: {e}") # Optional debug
        return float('inf')

def is_valid_P(P):
    """Checks if any column in P is the all-zero vector."""
    # Check if P is empty or dimensions are invalid before proceeding
    if P is None or P.size == 0 or P.ndim != 2:
         return False # Cannot be valid if not a proper 2D array
    k, p_cols = P.shape
    if p_cols == 0:
         return True # P has no columns, so no all-zero columns exist
    for j in range(p_cols):
        if np.all(P[:, j] == 0):
            return False
    return True

def get_p_matrix_from_index(index, k, p_cols, element_min, element_max):
    """
    Generates the P matrix corresponding to a specific index in the
    iteration space defined by element_min/max and dimensions k, p_cols.
    Maps a single integer index to a unique matrix configuration.
    """
    num_elements = k * p_cols
    range_size = element_max - element_min + 1

    # Handle edge cases
    if range_size <= 0:
        # print(f"Warning: Invalid element range size ({range_size}).")
        return None
    if num_elements == 0:
         # If k=0 or p_cols=0, P should be empty or have zero elements.
         # Return an appropriately shaped empty array.
         return np.empty((k, p_cols), dtype=np.int64)

    # Check if index is out of bounds for the total number of possibilities.
    # Be careful with large exponents.
    try:
        total_candidates = range_size ** num_elements
        if index >= total_candidates:
            # This indicates an issue with the calling logic if it happens within the intended loop range.
            # print(f"Warning: Index {index} is out of bounds ({total_candidates}).")
            return None
    except OverflowError:
        # If the total number itself overflows, we can't directly compare.
        # This scenario implies an extremely large search space.
        # The index calculation below might still work if index is representable.
        pass # Proceed with calculation, might still work for smaller indices

    elements = []
    temp_index = index
    for _ in range(num_elements):
        # Derives the value for each position based on the index in the 'range_size' base system.
        element_value = temp_index % range_size
        elements.append(element_min + element_value)
        temp_index //= range_size

    # The elements list is built from the 'least significant' element first,
    # based on the base conversion. Reverse it to match standard matrix filling order (e.g., row-major).
    elements.reverse()

    try:
      # Reshape the flat list of elements into the (k, p_cols) matrix.
      P = np.array(elements, dtype=np.int64).reshape((k, p_cols))
      return P
    except ValueError:
      # This error occurs if the number of elements generated doesn't match k * p_cols.
      # Should not happen with the current logic unless num_elements was calculated incorrectly.
      print(f"Error: Reshaping failed for index {index}. Num elements: {len(elements)}, Expected: {k*p_cols}")
      return None


def worker_function(worker_id, start_index, end_index, k, n, m, element_min, element_max, progress_interval):
    """Worker thread function to check a range of P matrix candidates."""
    global best_P_global, min_cost_global, lock
    # print(f"Worker {worker_id}: Checking indices [{start_index}, {end_index})") # Debug
    I_k = np.eye(k, dtype=np.int64) # Match dtype used in get_p_matrix_from_index
    p_cols = n - k
    local_best_P = None
    local_min_cost = float('inf')
    checked_count = 0

    for index in range(start_index, end_index):
        # Optional progress within worker (less frequent)
        if progress_interval > 0 and checked_count > 0 and checked_count % progress_interval == 0:
             with lock: # Update global under lock if better than current global
                 if local_min_cost < min_cost_global:
                      # Avoid printing too often, just update
                      min_cost_global = local_min_cost
                      best_P_global = local_best_P # Already copied below

        P = get_p_matrix_from_index(index, k, p_cols, element_min, element_max)

        if P is None: # Should not happen within the loop range normally
            # print(f"Worker {worker_id}: Warning - Failed to generate P for index {index}")
            continue # Skip this index if generation failed

        if not is_valid_P(P):
            continue # Skip if P has an all-zero column

        # Construct the full generator matrix G = [I|P]
        G = np.hstack((I_k, P))
        cost = calculate_cost(G, m) # Calculate m-height

        # Update the best result found *locally* within this thread
        if cost < local_min_cost:
            local_min_cost = cost
            local_best_P = P.copy() # Create a copy to avoid race conditions

        checked_count += 1

    # After checking all assigned indices, update the global best result
    # This reduces lock contention compared to updating on every improvement
    if local_best_P is not None:
        with lock: # Acquire lock to safely compare and update global variables
            if local_min_cost < min_cost_global:
                # Check again in case another thread updated global best in the meantime
                # print(f"Worker {worker_id}: Found new best cost {local_min_cost:.4f} (was {min_cost_global:.4f})") # Progress indicator
                min_cost_global = local_min_cost
                best_P_global = local_best_P # Assign the locally found best P
                # Optionally print the new best P here if needed for debugging
                # print("New best P:\n", best_P_global)

    # print(f"Worker {worker_id}: Finished. Checked {checked_count} valid candidates. Local best cost: {local_min_cost}") # Debug


def main():
    global best_P_global, min_cost_global, cost_counter # Allow modification

    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(
        description="Find the optimal systematic generator matrix G=[I|P] minimizing m-height using multi-threaded brute force search.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter # Show defaults in help
    )
    parser.add_argument('-k', type=int, required=True, help='Number of message bits (rows).')
    parser.add_argument('-n', type=int, required=True, help='Number of codeword bits (columns).')
    parser.add_argument('-m', type=int, required=True, help='m-height parameter (must be 1 <= m <= n-1).')
    parser.add_argument('--element-min', type=int, default=0, help='Minimum value for elements in P matrix.')
    parser.add_argument('--element-max', type=int, default=1, help='Maximum value for elements in P matrix.')
    # Default workers to CPU count, ensuring at least 1
    default_workers = max(1, os.cpu_count() or 1)
    parser.add_argument('--workers', type=int, default=default_workers,
                        help=f'Number of parallel worker threads.')

    args = parser.parse_args()

    K = args.k
    N = args.n
    M = args.m
    ELEMENT_MIN = args.element_min
    ELEMENT_MAX = args.element_max
    NUM_WORKERS = args.workers

    # --- Validate Parameters ---
    if K < 1: parser.error("k must be at least 1")
    if N <= K: parser.error("n must be greater than k for P matrix to exist")
    p_cols = N - K
    # p_cols can be 0 if N=K, but G=[I] in that case. The logic handles this.
    # if p_cols < 1: parser.error("n-k must be at least 1") # Allow n=k, P is empty
    if not (1 <= M <= N - 1): parser.error(f"m ({M}) must be between 1 and n-1 ({N-1})")
    if ELEMENT_MIN > ELEMENT_MAX: parser.error("element-min cannot be greater than element-max")
    if NUM_WORKERS < 1: parser.error("Number of workers must be at least 1")
    # Specific check: If range is only 0, and P exists (p_cols > 0), invalid P is guaranteed.
    if ELEMENT_MIN == 0 and ELEMENT_MAX == 0 and p_cols > 0:
         parser.error("Element range is [0, 0] and P matrix exists (n > k). Cannot satisfy no-all-zero column constraint.")

    # --- Calculate Search Space Size ---
    range_size = ELEMENT_MAX - ELEMENT_MIN + 1
    num_p_elements = K * p_cols
    total_candidates = 0

    if range_size <= 0:
        print("Error: Element range is empty or invalid (min > max).")
        sys.exit(1)
    elif num_p_elements == 0: # Case n=k, P is empty k x 0 matrix
         total_candidates = 1 # There's one matrix G=[I]
         # print("Note: n=k, the only possible matrix is G=[I_k].") # Suppress this note
    else:
        try:
            # Calculate total number of P matrices (before filtering zero columns)
            total_candidates = range_size ** num_p_elements
            # Check for potential overflow if numbers are massive
            if total_candidates == float('inf'):
                 print("Warning: Search space is extremely large (overflow detected).") # Keep warning
            # Provide a warning for large, but representable, search spaces. 10^12 is arbitrary.
            elif total_candidates > 10**12:
                 print(f"Warning: Search space is very large ({total_candidates:.2e} candidates). This will likely take a very long time.") # Keep warning, simplified
            elif total_candidates == 0: # Should only happen if range_size=0, already checked
                 print("Warning: Calculated zero candidates unexpectedly.") # Keep warning

        except OverflowError:
            print(f"Error: Search space size calculation ({range_size}^{num_p_elements}) resulted in overflow.") # Keep error
            print("Parameters (k, n, element range) are too large for brute force.")
            print("Consider reducing the element range or using the annealing generator for larger problems.")
            sys.exit(1)

    if total_candidates == 0 and num_p_elements > 0:
        print("No possible P matrices to check with the given parameters (excluding n=k case).") # Keep info
        sys.exit(0)

    # --- Start Search ---
    # Suppress these startup messages
    # print(f"Starting Brute Force Search for ({K}x{N}) matrix, m={M}-height.")
    # print(f"Element range for P: [{ELEMENT_MIN}, {ELEMENT_MAX}]")
    # if num_p_elements > 0:
    #     print(f"Total P matrix candidates to check (before zero-column filter): {total_candidates}")
    # print(f"Using {NUM_WORKERS} worker threads.")
    print("Starting brute force search... Press Ctrl+C to interrupt.") # Keep simple start message with interrupt info

    # --- Thread Setup ---
    threads = []
    # Ceiling division to distribute work as evenly as possible
    chunk_size = (total_candidates + NUM_WORKERS - 1) // NUM_WORKERS
    # Set progress update interval (e.g., check global best every X candidates)
    # Adjust based on expected runtime; larger interval reduces lock contention.
    progress_interval = max(1000, total_candidates // (NUM_WORKERS * 100)) # Heuristic

    start_time = time.time()

    for i in range(NUM_WORKERS):
        # Calculate the start and end index for this thread's chunk
        start_index = i * chunk_size
        end_index = min((i + 1) * chunk_size, total_candidates)

        # If start_index >= end_index, this thread has no work (can happen with few candidates)
        if start_index >= end_index:
            continue

        # Create and start the thread
        thread = threading.Thread(target=worker_function,
                                  args=(i, start_index, end_index, K, N, M, ELEMENT_MIN, ELEMENT_MAX, progress_interval))
        threads.append(thread)
        thread.start()

    # --- Wait for Threads and Display Progress ---
    try:
        # While any thread is alive, keep updating progress
        while any(t.is_alive() for t in threads):
            # Update progress based on the shared counter
            with cost_counter_lock: # Access counter safely
                 current_checked = cost_counter

            # Read global best cost safely under the main lock
            with lock:
                current_best_cost = min_cost_global

            elapsed_time = time.time() - start_time
            rate = current_checked / elapsed_time if elapsed_time > 0 else 0
            percent_complete = (current_checked / total_candidates) * 100 if total_candidates > 0 else 100

            # Format cost: display 'inf' or a number using the locked value
            cost_display = f"{current_best_cost:.4f}" if current_best_cost != float('inf') else "inf"

            # Write progress to the same line
            sys.stdout.write(
                f"\rProgress: {current_checked}/{total_candidates} ({percent_complete:.2f}%) | Rate: {rate:.1f}/s | Elapsed: {elapsed_time:.1f}s | Best Cost: {cost_display}   "
            )
            sys.stdout.flush()

            # Sleep briefly to avoid busy-waiting and reduce print frequency
            time.sleep(0.5)

        # Final join ensures all threads have completed their execution
        for thread in threads:
            thread.join()
        # Print a newline to move off the progress line cleanly
        print() # Ensures the next print starts on a new line after progress bar
        # print("All workers finished.") # Suppress this

    except KeyboardInterrupt:
        # Keep interrupt messages
        print("\nCtrl+C detected. Interrupting search.")
        print("Note: Threads will finish their current calculation.")
        print("The result shown will be the best found up to the interruption point.")
        # Threads will exit naturally after finishing their current loop iteration.
        # We join them briefly to ensure clean exit if possible.
        for thread in threads:
             thread.join(timeout=0.2) # Give threads a moment to finish/exit
        print("Threads joined or timed out.")


    # --- Final Report ---
    end_time = time.time()
    print(f"Search completed or interrupted in {end_time - start_time:.2f} seconds.") # Keep duration
    with cost_counter_lock: # Read final count safely
         print(f"Total cost function calls: {cost_counter}") # Keep total calls

    # --- Output Result ---
    if best_P_global is not None:
        print("\n--- Optimal Systematic Generator Matrix Found G = [I|P] ---") # Keep result header
        # Ensure I_k has the same dtype as the found P matrix
        I_k = np.eye(K, dtype=best_P_global.dtype)
        # Construct the best generator matrix G = [I|P]
        best_G = np.hstack((I_k, best_P_global))
        # Print the resulting matrix with controlled precision
        with np.printoptions(precision=3, suppress=True):
             print(best_G) # Keep result matrix
        # Print the minimal m-height found
        print(f"Minimal m-height (m={M}): {min_cost_global:.6g}") # Keep result cost

        # --- Optional Verification ---
        # Suppress verification steps output
        # print("\nVerifying final cost...")
        final_cost_check = calculate_cost(best_G, M) # Keep calculation for check below
        # print(f"Verification cost calculation: {final_cost_check:.6g}")
        if not math.isclose(final_cost_check, min_cost_global, rel_tol=1e-9, abs_tol=0.0):
             print(f"Warning: Final cost verification ({final_cost_check:.6g}) differs slightly from recorded best cost ({min_cost_global:.6g}).") # Keep warning if check fails
        # else:
             # print("Verification successful.") # Suppress success message

    else:
        # Keep messages for no result found
        print("\nNo valid systematic generator matrix found satisfying the constraints within the given range.")
        if total_candidates > 0:
             print("Possible reasons: all candidates had an all-zero column, or all resulted in errors/infinite cost during m-height calculation.")
        if M > N - K: # Check if m is too large for the code rate
             print(f"Note: m={M} might be too large relative to n-k={p_cols}, potentially leading to high or infinite costs.")


if __name__ == "__main__":
    # Note on Threading vs. Multiprocessing:
    # For CPU-bound tasks like NumPy/SciPy operations often involved in
    # compute_m_height, Python's Global Interpreter Lock (GIL) limits
    # true parallelism with threading. Multiprocessing typically yields
    # better performance by bypassing the GIL using separate processes.
    # However, this implementation uses threading as initially discussed.
    main()

-generator.py

import argparse
import subprocess
import sys
import os

def main():
    parser = argparse.ArgumentParser(
        description="Dispatcher script to generate a systematic generator matrix G=[I|P]. "
                    "Calls brute force for m <= 3 and adaptive annealing for m > 3.",
        # Allow unknown arguments to pass through to sub-scripts
        # argument_default=argparse.SUPPRESS # This might hide args from help, use parse_known_args instead
    )

    # Arguments common to both or potentially needed by annealing
    parser.add_argument('-k', type=int, default=3, help='Number of message bits (rows).')
    parser.add_argument('-n', type=int, default=6, help='Number of codeword bits (columns).')
    parser.add_argument('-m', type=int, default=2, help='m-height parameter.')

    # Arguments primarily for annealing, but might be used by brute force too
    parser.add_argument('--element-min', type=int, default=-5, help='Minimum value for elements in P matrix. Default: -100')
    parser.add_argument('--element-max', type=int, default=5, help='Maximum value for elements in P matrix. Default: 100')

    # Arguments specific to annealing_generator.py or adaptive_annealing.py
    parser.add_argument('--t-max', type=float, default=100, help='Initial annealing temperature.')
    parser.add_argument('--t-min', type=float, help='Final annealing temperature.')
    parser.add_argument('--alpha', type=float, default=0.95, help='Cooling rate (multiplier).')
    parser.add_argument('--iter-per-temp', type=int, default=100, help='Iterations per temperature level.')
    parser.add_argument('--workers', type=int, default=12, help='Number of parallel annealing runs.')
    
    # Arguments specific to adaptive model (currently unused, but keep for potential future use)
    # parser.add_argument('--strategy', type=str, choices=['heuristic', 'sampled', 'adaptive', 'auto'],
    #                   default='auto', help='Surrogate strategy for m > 3. Default: auto')
    # parser.add_argument('--sample-rate', type=float, default=0.1,
    #                   help='Sample rate for surrogate model (m > 3). Default: 0.1')
    # parser.add_argument('--verify-final', action='store_true',
    #                   help='Verify final result with exact method for m > 3')

    # New arguments for choosing method when m > 3 and for random search
    parser.add_argument('--large-m-method', type=str, choices=['annealing', 'random'], default='annealing',
                        help='Method to use when m > 3. Default: annealing')
    parser.add_argument('--num-samples', type=int, default=10000,
                        help='Number of random samples to check (used only if --large-m-method=random). Default: 10000')
    parser.add_argument('--seed', type=int, default=None,
                        help='Random seed for reproducibility (used only if --large-m-method=random).')

    # Parse known args first to decide the script, then pass remaining if necessary
    args, unknown_args = parser.parse_known_args()

    # Basic validation needed for dispatch logic
    if args.k is None or args.n is None or args.m is None:
        parser.error("Arguments -k, -n, and -m are required.")
    if args.n <= args.k:
        parser.error(f"n ({args.n}) must be greater than k ({args.k}).")
    if not (1 <= args.m <= args.n - 1):
         parser.error(f"m ({args.m}) must be between 1 and n-1 ({args.n-1}).")


    base_command = [sys.executable] # Use the same python interpreter

    if args.m <= 3:
        target_script = 'brute_force_generator.py'
        print(f"--- m = {args.m} <= 3, attempting to call {target_script} for exact search ---")
        base_command.append(target_script)

        # Add required arguments for brute force (assuming similar core args)
        base_command.extend(['-k', str(args.k)])
        base_command.extend(['-n', str(args.n)])
        base_command.extend(['-m', str(args.m)])

        # Add optional arguments if provided
        if args.element_min is not None:
            base_command.extend(['--element-min', str(args.element_min)])
        if args.element_max is not None:
            base_command.extend(['--element-max', str(args.element_max)])

        # Add any unknown arguments that might be specific to brute force
        base_command.extend(unknown_args)

        if not os.path.exists(target_script):
            print(f"Error: {target_script} not found. Please create this script.")
            sys.exit(1)

    else: # m > 3
        if args.large_m_method == 'annealing':
            target_script = 'annealing_generator.py'
            print(f"--- m = {args.m} > 3, method='annealing', calling {target_script} ---")
            base_command.append(target_script)

            # Add core arguments
            base_command.extend(['-k', str(args.k)])
            base_command.extend(['-n', str(args.n)])
            base_command.extend(['-m', str(args.m)])

            # Add optional arguments relevant to annealing
            if args.element_min is not None:
                base_command.extend(['--element-min', str(args.element_min)])
            if args.element_max is not None:
                base_command.extend(['--element-max', str(args.element_max)])
            if args.t_max is not None:
                base_command.extend(['--t-max', str(args.t_max)])
            if args.t_min is not None:
                base_command.extend(['--t-min', str(args.t_min)])
            if args.alpha is not None:
                base_command.extend(['--alpha', str(args.alpha)])
            if args.iter_per_temp is not None:
                base_command.extend(['--iter-per-temp', str(args.iter_per_temp)])
            if args.workers is not None:
                # Pass the workers argument to annealing script
                base_command.extend(['--workers', str(args.workers)])

            # Add any unknown arguments (might be relevant for annealing)
            base_command.extend(unknown_args)

        elif args.large_m_method == 'random':
            target_script = 'random_generator.py'
            print(f"--- m = {args.m} > 3, method='random', calling {target_script} ---")
            base_command.append(target_script)

            # Add core arguments
            base_command.extend(['-k', str(args.k)])
            base_command.extend(['-n', str(args.n)])
            base_command.extend(['-m', str(args.m)])

            # Add optional arguments relevant to random search
            if args.element_min is not None:
                base_command.extend(['--element-min', str(args.element_min)])
            if args.element_max is not None:
                base_command.extend(['--element-max', str(args.element_max)])
            if args.num_samples is not None:
                base_command.extend(['--num-samples', str(args.num_samples)])
            if args.seed is not None:
                base_command.extend(['--seed', str(args.seed)])
            # Note: progress_interval from random_generator is not exposed here, uses its default.
            # Note: workers arg from annealing is not used by random_generator (currently single-threaded)

            # Add any unknown arguments (less likely to be relevant for random, but pass just in case)
            base_command.extend(unknown_args)

        else:
             # This case should not be reachable due to 'choices' in argument definition
             parser.error(f"Unknown large-m-method: {args.large_m_method}")
             sys.exit(1) # Explicit exit after error

        # Check if the selected target script exists
        if not os.path.exists(target_script):
            print(f"Error: {target_script} not found. Please ensure the script exists.")
            sys.exit(1)

    print(f"Executing command: {' '.join(base_command)}")
    try:
        # Execute the command
        # Check=True raises CalledProcessError if the script exits with non-zero status
        # Capture_output=False lets the script print directly to console
        process = subprocess.run(base_command, check=True, text=True)
        print(f"--- {target_script} finished successfully ---")

    except FileNotFoundError:
        print(f"Error: The script '{target_script}' was not found.")
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"Error: {target_script} exited with status {e.returncode}.")
        # stderr might be None if capture_output=False, but error message is usually printed by the script itself
        sys.exit(e.returncode)
    except Exception as e:
        print(f"An unexpected error occurred while trying to run {target_script}: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

- manual_verify.py

from verifier import compute_m_height
import numpy as np

# This function is used to manually verify the minimal m-height of a given generator matrix

# Generate a wider 3x8 generator matrix to allow larger m
# G = np.array([
#     [1, 0, 0, 1, 2, -1, 1, 0],
#     [0, 1, 0, -1, 1, 2, 0, 1],
#     [0, 0, 1, 2, 1, -1, 1, -1]
# ])
# Create a 4x4 identity matrix
# Create a 5x5 identity matrix
identity = np.eye(5)

# Combine with the given matrix to form a 5x9 generator matrix
G = np.hstack((identity, np.array([
    [1, 1, 1, 1],
    [1, 1, 1, 0],
    [1, 1, 0, 1],
    [1, 1, 0, 0],
    [1, 0, 1, 0]
])))

# Set m to 2 (since n - k = 9 - 5 = 4, so m=2 is valid)
m = 2

# Test the verifier with wider matrix
try:
    minimal_height = compute_m_height(G, m)
    print(f"Minimal m-height for matrix:\n{G}\nwith m={m} is: {minimal_height}")
except ValueError as e:
    print(f"Error: {e}")

- process_matrices.py

import numpy as np
import pickle
import json
import re

# Replace these with your information
NAME = "Yuuzen_Shen"
UIN = "434000618"
SESSION = "629-601"  # Change to your session (e.g., "629-601" or "629-700")
PROJECT = "P1"  # Change to "P1", "P2", or "P3"

def read_matrices_from_file(filename):
    generator_matrices = {}
    heights = {}
    
    with open(filename, 'r') as file:
        content = file.read()
        
    # Split the content into blocks for each matrix
    pattern = r'(\d+ \d+ \d+\n(?:[-\d ]++\n)+(?:inf|[\d.]+))'
    blocks = re.findall(pattern, content)
    
    for block in blocks:
        lines = block.strip().split('\n')
        n, k, m = map(int, lines[0].split())
        
        # The key is the string version of [n, k, m]
        key = json.dumps([n, k, m])
        
        # Parse the matrix rows (next k lines)
        matrix_rows = []
        for i in range(1, k+1):
            row = list(map(int, lines[i].split()))
            matrix_rows.append(row)
        
        # Convert to numpy array
        matrix = np.array(matrix_rows)
        
        # Validate matrix
        if matrix.shape[1] != n-k:
            print(f"Warning: Matrix for {[n, k, m]} has incorrect dimensions. Expected {k}x{n-k}, got {matrix.shape}")
            continue
            
        # Check if any values are outside the range [-100, 100]
        if np.any(matrix < -100) or np.any(matrix > 100):
            print(f"Warning: Matrix for {[n, k, m]} contains values outside the range [-100, 100]")
            continue
            
        # Check if any column is all zeros
        if np.any(np.all(matrix == 0, axis=0)):
            print(f"Warning: Matrix for {[n, k, m]} has columns that are all zeros")
            continue
        
        # Get m-height (the last line)
        try:
            height = float(lines[k+1])
            if height == float('inf'):
                print(f"Warning: Matrix for {[n, k, m]} has infinite m-height, skipping")
                continue
                
            if height < 1:
                print(f"Warning: Matrix for {[n, k, m]} has m-height less than 1 ({height}), skipping")
                continue
        except ValueError:
            print(f"Warning: Could not parse m-height for {[n, k, m]}")
            continue
        
        # Add to dictionaries
        generator_matrices[key] = matrix
        heights[key] = height
    
    return generator_matrices, heights

def main():
    generator_matrices, heights = read_matrices_from_file('final_matrix.txt')
    
    # Print summary
    print(f"Found {len(generator_matrices)} valid matrices")
    for key in generator_matrices:
        params = json.loads(key)
        print(f"Matrix {params}: {generator_matrices[key].shape}, m-height: {heights[key]}")
    
    # Save dictionaries to files
    generator_filename = f"generatorMatrix-{NAME}-{UIN}-{SESSION}-SP25-{PROJECT}"
    height_filename = f"mHeight-{NAME}-{UIN}-{SESSION}-SP25-{PROJECT}"
    
    with open(generator_filename, 'wb') as f:
        pickle.dump(generator_matrices, f)
    
    with open(height_filename, 'wb') as f:
        pickle.dump(heights, f)
    
    print(f"Saved generator matrices to {generator_filename}")
    print(f"Saved m-heights to {height_filename}")

if __name__ == "__main__":
    main()

- random_generator.py

import numpy as np
import argparse
import sys
import random
import time
import os
import concurrent.futures
import threading
import multiprocessing
from tqdm import tqdm

try:
    # Assuming compute_m_height is in verifier.py at the same level
    from verifier import compute_m_height
except ImportError:
    print("Error: Could not import 'compute_m_height' from 'verifier.py'.")
    print("Ensure 'verifier.py' exists and is in the same directory or Python path.")
    sys.exit(1)

# We'll use a Manager for sharing variables across processes
manager = multiprocessing.Manager()
# Shared variables for tracking best result
shared_dict = manager.dict()
shared_dict['best_P_global'] = None
shared_dict['min_height_global'] = float('inf')
shared_dict['checked_count'] = 0
# Lock for multiprocessing
shared_lock = manager.Lock()
# Progress bar - will be initialized in main process
progress_bar = None

def is_valid_P(P):
    """Checks if any column in P is the all-zero vector."""
    if P.shape[1] == 0: # Handle case n=k, P is empty
        return True
    k, p_cols = P.shape
    for j in range(p_cols):
        if np.all(P[:, j] == 0):
            return False
    return True

def generate_random_P(k, p_cols, element_min, element_max):
    """Generates a random P matrix with integer elements in the specified range."""
    # Ensure the range is valid
    if element_min > element_max:
        raise ValueError("element_min cannot be greater than element_max")
    return np.random.randint(element_min, element_max + 1, size=(k, p_cols), dtype=np.int64)

def calculate_cost(G, m):
    """Calculates the m-height, handling potential errors or infinite results."""
    with shared_lock:
        shared_dict['checked_count'] += 1
        # Update progress bar if it exists (in the main process)
        if 'main_pid' in shared_dict and shared_dict['main_pid'] == os.getpid() and progress_bar is not None:
            progress_bar.update(1)
    
    try:
        height = compute_m_height(G, m)
        # Ensure infinity is returned consistently if calculation fails or result is inf
        return height if np.isfinite(height) else float('inf')
    except Exception as e:
        # print(f"Warning: compute_m_height failed. Error: {e}") # Optional debug
        return float('inf')

def update_progress_bar_description():
    """Updates the progress bar description with current stats."""
    if progress_bar is None or 'main_pid' not in shared_dict or shared_dict['main_pid'] != os.getpid():
        return
        
    elapsed_time = time.time() - shared_dict['start_time']
    rate = shared_dict['checked_count'] / elapsed_time if elapsed_time > 0 else 0
    current_best = f"{shared_dict['min_height_global']:.4f}" if shared_dict['min_height_global'] != float('inf') else "inf"
    
    progress_bar.set_description(
        f"Rate: {rate:.2f}/s | Best: {current_best}"
    )

def process_sample(args):
    """Process a single random matrix sample. This function will run in a separate process."""
    i, K, p_cols, ELEMENT_MIN, ELEMENT_MAX, M, I_k_list = args
    
    # Reconstruct the identity matrix (numpy arrays aren't directly picklable)
    I_k = np.array(I_k_list, dtype=np.int64).reshape(K, K)
    
    # Each process should have its own random state
    local_random = np.random.RandomState(os.getpid() + i)
    
    # Generate a random P matrix using local random state
    P = local_random.randint(ELEMENT_MIN, ELEMENT_MAX + 1, size=(K, p_cols), dtype=np.int64)

    # Check if P is valid (no all-zero columns)
    if not is_valid_P(P):
        return None  # Skip invalid P
    
    # Construct the full generator matrix G = [I|P]
    G = np.hstack((I_k, P))

    # Print information before calculating cost
    print(f"Checking sample {i}...")

    # Calculate m-height cost
    cost = calculate_cost(G, M)
    
    # Update the best result found so far if needed
    with shared_lock:
        if cost < shared_dict['min_height_global']:
            shared_dict['min_height_global'] = cost
            # We need to store P as a list to make it picklable for the manager
            shared_dict['best_P_global'] = P.tolist()
            
            # Only update progress bar from main process
            if 'main_pid' in shared_dict and shared_dict['main_pid'] == os.getpid():
                update_progress_bar_description()
    
    return cost

def main():
    global progress_bar

    # --- Argument Parsing ---
    parser = argparse.ArgumentParser(
        description="Find a systematic generator matrix G=[I|P] with low m-height using random search.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter # Show defaults in help
    )
    parser.add_argument('-k', type=int, required=True, help='Number of message bits (rows).')
    parser.add_argument('-n', type=int, required=True, help='Number of codeword bits (columns).')
    parser.add_argument('-m', type=int, required=True, help='m-height parameter (must be 1 <= m <= n-1).')
    parser.add_argument('--element-min', type=int, default=-1, help='Minimum value for elements in P matrix.')
    parser.add_argument('--element-max', type=int, default=1, help='Maximum value for elements in P matrix.')
    parser.add_argument('--num-samples', type=int, default=100, help='Number of random P matrices to check.')
    parser.add_argument('--seed', type=int, default=None, help='Random seed for reproducibility.')
    parser.add_argument('--num-processes', type=int, default=None, 
                        help='Number of processes to use. Default is number of CPU cores.')
    parser.add_argument('--no-progress-bar', action='store_true', 
                        help='Disable the progress bar.')


    args = parser.parse_args()

    K = args.k
    N = args.n
    M = args.m
    ELEMENT_MIN = args.element_min
    ELEMENT_MAX = args.element_max
    NUM_SAMPLES = args.num_samples
    RANDOM_SEED = args.seed
    NUM_PROCESSES = args.num_processes or os.cpu_count()
    SHOW_PROGRESS_BAR = not args.no_progress_bar

    # Set random seed if provided
    if RANDOM_SEED is not None:
        np.random.seed(RANDOM_SEED)
        random.seed(RANDOM_SEED) # Also seed Python's random for other potential uses

    # --- Validate Parameters ---
    if K < 1: parser.error("k must be at least 1")
    if N <= K: parser.error("n must be greater than k for P matrix to exist")
    p_cols = N - K
    if not (1 <= M <= N - 1): parser.error(f"m ({M}) must be between 1 and n-1 ({N-1})")
    if ELEMENT_MIN > ELEMENT_MAX: parser.error("element-min cannot be greater than element-max")
    if NUM_SAMPLES < 1: parser.error("Number of samples must be at least 1")
    if NUM_PROCESSES < 1: parser.error("Number of processes must be at least 1")
    # Specific check: If range is only 0, and P exists (p_cols > 0), invalid P is guaranteed.
    if ELEMENT_MIN == 0 and ELEMENT_MAX == 0 and p_cols > 0:
         parser.error("Element range is [0, 0] and P matrix exists (n > k). Cannot satisfy no-all-zero column constraint.")

    # --- Start Search ---
    print(f"Starting Random Search for ({K}x{N}) matrix, m={M}-height.")
    print(f"Checking {NUM_SAMPLES} random P matrices using {NUM_PROCESSES} processes.")
    print(f"Element range for P: [{ELEMENT_MIN}, {ELEMENT_MAX}]")
    if RANDOM_SEED is not None:
        print(f"Using random seed: {RANDOM_SEED}")
    
    I_k = np.eye(K, dtype=np.int64) # Identity matrix part
    
    # Store I_k as a list for pickling (multiprocessing needs serializable data)
    I_k_list = I_k.tolist()
    
    # Set up shared variables
    shared_dict['start_time'] = time.time()
    shared_dict['main_pid'] = os.getpid()

    try:
        # Prepare arguments for each sample
        sample_args = [(i, K, p_cols, ELEMENT_MIN, ELEMENT_MAX, M, I_k_list) 
                       for i in range(NUM_SAMPLES)]
        
        # Initialize progress bar if enabled
        if SHOW_PROGRESS_BAR:
            progress_bar = tqdm(total=NUM_SAMPLES, unit="matrices", 
                               desc="Rate: 0.00/s | Best: inf", ncols=100)
        
        # Use ProcessPoolExecutor for multiprocessing
        with concurrent.futures.ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:
            # Submit all tasks and get futures
            futures = [executor.submit(process_sample, arg) for arg in sample_args]
            
            # Wait for all tasks to complete or for a KeyboardInterrupt
            try:
                # Process results as they complete to update progress bar
                for future in concurrent.futures.as_completed(futures):
                    # Just ensuring the future completes
                    result = future.result()
                    # Update progress description periodically
                    if SHOW_PROGRESS_BAR and shared_dict['checked_count'] % max(1, NUM_SAMPLES//100) == 0:
                        update_progress_bar_description()
                        
            except KeyboardInterrupt:
                print("\nSearch interrupted by user. Shutting down processes...")
                executor.shutdown(wait=False)
                raise KeyboardInterrupt

    except KeyboardInterrupt:
        print("\nSearch interrupted by user.")

    finally:
        # Clean up the progress bar
        if progress_bar is not None:
            progress_bar.close()
            
        end_time = time.time()
        print(f"Search finished in {end_time - shared_dict['start_time']:.2f} seconds.")
        print(f"Total matrices checked: {shared_dict['checked_count']}")

        # --- Output Result ---
        if shared_dict['best_P_global'] is not None:
            print("--- Best Systematic Generator Matrix Found G = [I|P] ---")
            # Reconstruct the best P matrix
            best_P = np.array(shared_dict['best_P_global'], dtype=np.int64)
            # Construct the best generator matrix G = [I|P]
            best_G = np.hstack((I_k, best_P))
            # Print the resulting matrix with controlled precision
            with np.printoptions(precision=3, suppress=True):
                 print(best_G)
            # Print the minimal m-height found
            print(f"Minimal m-height (m={M}): {shared_dict['min_height_global']:.6g}")

            # --- Optional Verification ---
            # print("Verifying final cost...")
            final_cost_check = calculate_cost(best_G, M) # Rerun calculation
            # print(f"Verification cost calculation: {final_cost_check:.6g}")
            if not np.isclose(final_cost_check, shared_dict['min_height_global'], equal_nan=True):
                 print(f"Warning: Final cost verification ({final_cost_check:.6g}) differs from recorded best cost ({shared_dict['min_height_global']:.6g}).")
            # else:
            #      print("Verification successful.")
        else:
            print("No valid systematic generator matrix found with finite m-height.")
            if shared_dict['checked_count'] < NUM_SAMPLES:
                 print("Note: Not all samples were fully checked (some might have been invalid or interrupted).")

if __name__ == "__main__":
    multiprocessing.freeze_support()  # For Windows support when compiled
    main()

- seed_annealing_generator.py

import numpy as np
import argparse
import sys
import random
import time
import os
import concurrent.futures
import multiprocessing
import math
import copy
from tqdm import tqdm

try:
    # Assuming compute_m_height is in verifier.py at the same level
    from verifier import compute_m_height
except ImportError:
    print("Error: Could not import 'compute_m_height' from 'verifier.py'.")
    print("Please ensure 'verifier.py' exists in the same directory or in the Python path.")
    sys.exit(1)

# --- Helper Functions ---

def load_p_matrix(filepath):
    """
    Loads n, k, m, and the P matrix from a text file.
    Expected format:
    n k m
    p11 p12 ... p1(n-k)
    p21 p22 ... p2(n-k)
    ...
    pk1 pk2 ... pk(n-k)
    """
    try:
        with open(filepath, 'r') as f:
            # Read the first line for n, k, m
            first_line = f.readline().strip()
            try:
                n, k, m = map(int, first_line.split())
            except ValueError:
                raise ValueError("First line must contain three integers: n, k, m, separated by spaces.")

            # Basic validation of n, k, m
            if not (k >= 1):
                raise ValueError("k must be >= 1.")
            if not (n > k):
                raise ValueError("n must be > k.")
            p_cols = n - k
            if not (1 <= m <= n - 1):
                 raise ValueError(f"m ({m}) must be between 1 and n-1 ({n-1}).")

            # Read the rest of the lines for the P matrix
            # Use np.loadtxt on the rest of the file stream
            P = np.loadtxt(f, dtype=np.int64)

            # Validate P matrix dimensions
            if P.ndim == 0:
                 raise ValueError("Matrix data appears empty or scalar after the first line.")
            elif P.ndim == 1:
                # If P has only one row (k=1), loadtxt might return a 1D array.
                # Or if p_cols is 1, it might also be 1D.
                if k == 1 and P.shape[0] == p_cols:
                     P = P.reshape(1, p_cols)
                elif p_cols == 1 and P.shape[0] == k:
                     P = P.reshape(k, 1)
                else:
                    # Ambiguous or incorrect shape
                    raise ValueError(f"Loaded matrix data has unexpected 1D shape {P.shape}. Expected ({k}, {p_cols}).")
            elif P.shape != (k, p_cols):
                raise ValueError(f"Loaded P matrix shape {P.shape} does not match dimensions from first line ({k}x{p_cols}).")

        print(f"Loaded n={n}, k={k}, m={m} and seed P matrix from '{filepath}' with shape: {P.shape}")
        return n, k, m, P

    except FileNotFoundError:
        print(f"Error: Seed file not found at '{filepath}'")
        sys.exit(1)
    except Exception as e:
        print(f"Error: Could not load data from '{filepath}': {e}")
        sys.exit(1)

def save_p_matrix(matrix, filepath):
    """Saves the P matrix to a text file."""
    try:
        np.savetxt(filepath, matrix, fmt='%d')
        print(f"Best P matrix saved to '{filepath}'")
    except Exception as e:
        print(f"Error: Could not save P matrix to '{filepath}': {e}")

def is_valid_P(P):
    """Checks if any column in P is the all-zero vector."""
    if P.shape[1] == 0: # Handle case n=k, P is empty
        return True
    k, p_cols = P.shape
    for j in range(p_cols):
        if np.all(P[:, j] == 0):
            return False
    return True

def calculate_cost(G, m):
    """Calculates the m-height, handling potential errors or infinite results."""
    try:
        height = compute_m_height(G, m)
        return height if np.isfinite(height) else float('inf')
    except Exception as e:
        # print(f"Warning: compute_m_height failed. Error: {e}") # Optional debug info
        return float('inf')

def generate_neighbor(current_P, element_min, element_max):
    """
    Generates a neighboring P matrix.
    Strategy: Randomly select an element in P and change it to a new random value within the range.
    Ensures the modified column is not the all-zero vector.
    """
    k, p_cols = current_P.shape
    if p_cols == 0:
        return current_P.copy() # Cannot generate neighbor if P is empty

    neighbor_P = current_P.copy()
    max_attempts = k * p_cols * 5 # Set an attempt limit to avoid infinite loops

    for _ in range(max_attempts):
        row_idx = random.randrange(k)
        col_idx = random.randrange(p_cols)
        original_value = neighbor_P[row_idx, col_idx]

        # Try different new values
        for _ in range(10): # Try a few times to get a different value
            new_value = random.randint(element_min, element_max)
            if new_value != original_value:
                break
        else:
            # May not be possible to change if the range is very small (e.g., only one value)
            new_value = random.randint(element_min, element_max)

        neighbor_P[row_idx, col_idx] = new_value

        # Check if the modified column is all zeros
        if np.all(neighbor_P[:, col_idx] == 0):
            # If all zeros, revert the change and try a different position
            neighbor_P[row_idx, col_idx] = original_value
            continue # Try modifying another element
        else:
            # Found a valid neighbor
            return neighbor_P

    # If no valid neighbor found after many attempts (very unlikely unless range is very restricted)
    print("Warning: Could not generate a valid neighbor P matrix (avoiding all-zero column). Returning current P.")
    return current_P.copy()

def evaluate_candidate(args):
    """Evaluates the cost of a single candidate P matrix (runs in a separate process)."""
    P_candidate_list, k, m, I_k_list = args
    try:
        P_candidate = np.array(P_candidate_list, dtype=np.int64)
        I_k = np.array(I_k_list, dtype=np.int64).reshape(k,k)
        G_candidate = np.hstack((I_k, P_candidate))
        cost = calculate_cost(G_candidate, m)
        return P_candidate.tolist(), cost
    except Exception as e:
        # print(f"Error evaluating candidate matrix: {e}") # Optional debug
        return P_candidate_list, float('inf')


# --- Simulated Annealing Core ---

def simulated_annealing(initial_P, m, element_min, element_max,
                         initial_temp, cooling_rate, steps_per_temp,
                         num_workers, output_file):
    """Performs the simulated annealing algorithm."""
    k, p_cols = initial_P.shape
    n = k + p_cols
    I_k = np.eye(k, dtype=np.int64)
    I_k_list = I_k.tolist() # For pickling

    current_P = initial_P.copy()
    current_G = np.hstack((I_k, current_P))
    current_cost = 114514  # Set to infinite to ignore seed's initial cost

    best_P = current_P.copy()
    best_cost = current_cost

    second_best_P = current_P.copy()
    second_best_cost = current_cost

    print(f"Initial seed cost (m={m}): {current_cost if current_cost != float('inf') else 'inf'}")
    if best_cost == float('inf'):
        print("Warning: Initial seed matrix has infinite cost. Annealing might not be effective.")


    temperature = initial_temp
    # Estimate total steps for progress bar based on when temperature drops below 0.01
    total_steps = int(steps_per_temp * (math.log(0.01 / initial_temp) / math.log(cooling_rate) if cooling_rate < 1 and initial_temp > 0.01 else 10))

    progress_bar = tqdm(total=total_steps, unit="steps", desc="Annealing", ncols=100)
    steps_done = 0

    try:
        with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:
            while temperature > 0.01: # Stopping condition
                futures = []
                # Generate and evaluate neighbors in parallel at each temperature
                tasks_at_this_temp = []
                for _ in range(steps_per_temp):
                     # Generate a neighbor
                    neighbor_P = generate_neighbor(current_P, element_min, element_max)
                    if not is_valid_P(neighbor_P): # Double-check validity just in case
                        continue
                    tasks_at_this_temp.append((neighbor_P.tolist(), k, m, I_k_list))

                # Submit evaluation tasks
                # Using executor.map for potentially better memory usage with many tasks
                results = executor.map(evaluate_candidate, tasks_at_this_temp)

                # Process results and apply SA logic
                for neighbor_P_list, neighbor_cost in results:
                    steps_done += 1
                    progress_bar.update(1)

                    neighbor_P = np.array(neighbor_P_list, dtype=np.int64)

                    if neighbor_cost < current_cost:
                        # Accept better solution
                        current_P = neighbor_P
                        current_cost = neighbor_cost
                        # Update the global best solution
                        if neighbor_cost < best_cost:
                            # Update second best to the previous best
                            second_best_P = best_P.copy()
                            second_best_cost = best_cost
                            best_cost = neighbor_cost
                            best_P = neighbor_P.copy()
                            progress_bar.set_description(f"Annealing (T={temperature:.2f}, Best={best_cost:.4f})")
                        elif neighbor_cost < second_best_cost:
                            # Update second best to the current solution
                            second_best_P = neighbor_P.copy()
                            second_best_cost = neighbor_cost

                    else:
                        # Accept worse solution with a certain probability
                        delta_cost = neighbor_cost - current_cost
                        acceptance_prob = math.exp(-delta_cost / temperature)
                        if random.random() < acceptance_prob:
                            current_P = neighbor_P
                            current_cost = neighbor_cost

                # Cool down
                temperature *= cooling_rate
                progress_bar.set_description(f"Annealing (T={temperature:.2f}, Best={best_cost:.4f})")


    except KeyboardInterrupt:
        print("Annealing process interrupted by user.")
        print("Returning the second best solution found so far.")
        print(second_best_P)
        print(f"Best cost found before interruption: {best_cost if best_cost != float('inf') else 'inf'}")
        return second_best_P, second_best_cost
    finally:
        progress_bar.close()

    print("--- Simulated Annealing Finished ---")
    print(f"Best cost found (m={m}): {best_cost if best_cost != float('inf') else 'inf'}")
    print(f"Second best cost found (m={m}): {second_best_cost if second_best_cost != float('inf') else 'inf'}")

    if second_best_cost < float('inf'):
         # Save the second best result
        if output_file:
            save_p_matrix(second_best_P, output_file)
        else:
            print("Second best P matrix:")
            with np.printoptions(precision=3, suppress=True):
                print(second_best_P)
            print("Corresponding G = [I|P] matrix:")
            second_best_G = np.hstack((I_k, second_best_P))
            with np.printoptions(precision=3, suppress=True):
                 print(second_best_G)

    return second_best_P, second_best_cost


# --- Main Execution ---

def main():
    parser = argparse.ArgumentParser(
        description="Optimize the m-height of a systematic generator matrix G=[I|P] using simulated annealing, starting from a seed file containing n, k, m, and the P matrix.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('--seed-file', type=str, default="test.txt",
                        help='Path to the text file containing n, k, m on the first line, followed by the initial seed P matrix.')
    parser.add_argument('--initial-temp', type=float, default=100,
                        help='Initial temperature for simulated annealing.')
    parser.add_argument('--cooling-rate', type=float, default=0.9,
                        help='Cooling rate (0 < rate < 1).')
    parser.add_argument('--steps-per-temp', type=int, default=5,
                        help='Number of steps (neighbor evaluations) to perform at each temperature.')
    parser.add_argument('--num-workers', type=int, default=None,
                        help='Number of worker processes for parallel neighbor evaluation. Defaults to number of CPU cores.')
    parser.add_argument('--output-file', type=str, default=None,
                        help='Path to save the best found P matrix. If not provided, prints to console.')
    parser.add_argument('--seed', type=int, default=None, help='Random seed for reproducibility.')

    args = parser.parse_args()

    # Set random seed
    if args.seed is not None:
        random.seed(args.seed)
        np.random.seed(args.seed) # Ensure numpy's randomness is also seeded

    # Load n, k, m, and initial P matrix from the file
    n, k, m, initial_P = load_p_matrix(args.seed_file)
    p_cols = n - k # Calculate p_cols from loaded n and k

    # Calculate element_min and element_max based on the seed matrix
    element_min = int(np.min(initial_P))
    element_max = int(np.max(initial_P))

    # --- Validate Parameters (some validations are now done in load_p_matrix) ---
    # k, n, m validation is implicitly done by load_p_matrix success
    if element_min > element_max: parser.error("element-min cannot be greater than element-max")
    if args.initial_temp <= 0: parser.error("Initial temperature must be positive")
    if not (0 < args.cooling_rate < 1): parser.error("Cooling rate must be between 0 and 1")
    if args.steps_per_temp < 1: parser.error("Steps per temperature must be at least 1")

    num_workers = args.num_workers or os.cpu_count()
    if num_workers < 1: parser.error("Number of worker processes must be at least 1")

     # Specific check for element range and zero columns
    if element_min == 0 and element_max == 0 and p_cols > 0:
         parser.error("Element range is [0, 0] and P matrix exists (n > k). Cannot satisfy no-all-zero column constraint during neighbor search.")

    # Check initial P validity loaded from file
    if not is_valid_P(initial_P):
        print("Warning: The loaded seed P matrix contains at least one all-zero column.")
        # Decide if this should be an error or just a warning. For SA, it might be okay to start invalid.
        # parser.error("Loaded seed P matrix has an all-zero column, which is invalid.")


    print(f"--- Starting Simulated Annealing ---")
    print(f"Parameters from file: k={k}, n={n}, m={m}")
    print(f"Search Element Range: [{element_min}, {element_max}]")
    print(f"Annealing Params -> Initial Temp: {args.initial_temp}, Cooling Rate: {args.cooling_rate}, Steps/Temp: {args.steps_per_temp}")
    print(f"Using {num_workers} worker processes")
    if args.seed is not None: print(f"Random Seed: {args.seed}")


    start_time = time.time()

    # Execute simulated annealing - pass m read from the file
    best_P, best_cost = simulated_annealing(
        initial_P=initial_P,
        m=m, # Use m from the file
        element_min=element_min,
        element_max=element_max,
        initial_temp=args.initial_temp,
        cooling_rate=args.cooling_rate,
        steps_per_temp=args.steps_per_temp,
        num_workers=num_workers,
        output_file=args.output_file
    )

    end_time = time.time()
    print(f"Total time: {end_time - start_time:.2f} seconds")


if __name__ == "__main__":
    multiprocessing.freeze_support()  # For Windows support when compiled
    main()